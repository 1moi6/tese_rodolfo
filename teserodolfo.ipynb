{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "original = pd.read_csv(\"dados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "modificado = pd.DataFrame()\n",
    "for x in original:\n",
    "    if original[x].dtypes==object:\n",
    "        modificado[x] = le.fit_transform(np.array(original[x]))\n",
    "    else:\n",
    "        modificado[x] = np.array(original[x])\n",
    "\n",
    "clm = list(modificado.columns)\n",
    "X = np.array(modificado[clm[0:-1]])\n",
    "y = np.array(modificado[clm[-1]])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.34731306\n",
      "Iteration 2, loss = 0.26261527\n",
      "Iteration 3, loss = 0.25873050\n",
      "Iteration 4, loss = 0.25611325\n",
      "Iteration 5, loss = 0.25259873\n",
      "Iteration 6, loss = 0.24992609\n",
      "Iteration 7, loss = 0.24857737\n",
      "Iteration 8, loss = 0.24747715\n",
      "Iteration 9, loss = 0.24679689\n",
      "Iteration 10, loss = 0.24628322\n",
      "Iteration 11, loss = 0.24583270\n",
      "Iteration 12, loss = 0.24536416\n",
      "Iteration 13, loss = 0.24498747\n",
      "Iteration 14, loss = 0.24478481\n",
      "Iteration 15, loss = 0.24505096\n",
      "Iteration 16, loss = 0.24433604\n",
      "Iteration 17, loss = 0.24438875\n",
      "Iteration 18, loss = 0.24450217\n",
      "Iteration 19, loss = 0.24383770\n",
      "Iteration 20, loss = 0.24372044\n",
      "Iteration 21, loss = 0.24350176\n",
      "Iteration 22, loss = 0.24340314\n",
      "Iteration 23, loss = 0.24339981\n",
      "Iteration 24, loss = 0.24315664\n",
      "Iteration 25, loss = 0.24293998\n",
      "Iteration 26, loss = 0.24318429\n",
      "Iteration 27, loss = 0.24276112\n",
      "Iteration 28, loss = 0.24271919\n",
      "Iteration 29, loss = 0.24270080\n",
      "Iteration 30, loss = 0.24241451\n",
      "Iteration 31, loss = 0.24228889\n",
      "Iteration 32, loss = 0.24215307\n",
      "Iteration 33, loss = 0.24208294\n",
      "Iteration 34, loss = 0.24216405\n",
      "Iteration 35, loss = 0.24243190\n",
      "Iteration 36, loss = 0.24219151\n",
      "Iteration 37, loss = 0.24160027\n",
      "Iteration 38, loss = 0.24174851\n",
      "Iteration 39, loss = 0.24166411\n",
      "Iteration 40, loss = 0.24133734\n",
      "Iteration 41, loss = 0.24152946\n",
      "Iteration 42, loss = 0.24118896\n",
      "Iteration 43, loss = 0.24106782\n",
      "Iteration 44, loss = 0.24106885\n",
      "Iteration 45, loss = 0.24096059\n",
      "Iteration 46, loss = 0.24134593\n",
      "Iteration 47, loss = 0.24069931\n",
      "Iteration 48, loss = 0.24053507\n",
      "Iteration 49, loss = 0.24064585\n",
      "Iteration 50, loss = 0.24176505\n",
      "Iteration 51, loss = 0.24059885\n",
      "Iteration 52, loss = 0.24023649\n",
      "Iteration 53, loss = 0.24028556\n",
      "Iteration 54, loss = 0.24000666\n",
      "Iteration 55, loss = 0.24040289\n",
      "Iteration 56, loss = 0.23994574\n",
      "Iteration 57, loss = 0.23978959\n",
      "Iteration 58, loss = 0.23967767\n",
      "Iteration 59, loss = 0.23976116\n",
      "Iteration 60, loss = 0.24038957\n",
      "Iteration 61, loss = 0.23975599\n",
      "Iteration 62, loss = 0.23925953\n",
      "Iteration 63, loss = 0.23994392\n",
      "Iteration 64, loss = 0.24005494\n",
      "Iteration 65, loss = 0.23935846\n",
      "Iteration 66, loss = 0.23911364\n",
      "Iteration 67, loss = 0.23898124\n",
      "Iteration 68, loss = 0.23904474\n",
      "Iteration 69, loss = 0.23909029\n",
      "Iteration 70, loss = 0.23897852\n",
      "Iteration 71, loss = 0.23866140\n",
      "Iteration 72, loss = 0.23873527\n",
      "Iteration 73, loss = 0.23852016\n",
      "Iteration 74, loss = 0.23903335\n",
      "Iteration 75, loss = 0.23866421\n",
      "Iteration 76, loss = 0.23847008\n",
      "Iteration 77, loss = 0.23870314\n",
      "Iteration 78, loss = 0.23818730\n",
      "Iteration 79, loss = 0.23875511\n",
      "Iteration 80, loss = 0.23888032\n",
      "Iteration 81, loss = 0.23865873\n",
      "Iteration 82, loss = 0.23814541\n",
      "Iteration 83, loss = 0.23810090\n",
      "Iteration 84, loss = 0.23809857\n",
      "Iteration 85, loss = 0.23786660\n",
      "Iteration 86, loss = 0.23834403\n",
      "Iteration 87, loss = 0.23801221\n",
      "Iteration 88, loss = 0.23764812\n",
      "Iteration 89, loss = 0.23801200\n",
      "Iteration 90, loss = 0.23754120\n",
      "Iteration 91, loss = 0.23815376\n",
      "Iteration 92, loss = 0.23762335\n",
      "Iteration 93, loss = 0.23727252\n",
      "Iteration 94, loss = 0.23734116\n",
      "Iteration 95, loss = 0.23762537\n",
      "Iteration 96, loss = 0.23740984\n",
      "Iteration 97, loss = 0.23730378\n",
      "Iteration 98, loss = 0.23708036\n",
      "Iteration 99, loss = 0.23696014\n",
      "Iteration 100, loss = 0.23693901\n",
      "Iteration 101, loss = 0.23692779\n",
      "Iteration 102, loss = 0.23694581\n",
      "Iteration 103, loss = 0.23693109\n",
      "Iteration 104, loss = 0.23693605\n",
      "Iteration 105, loss = 0.23680403\n",
      "Iteration 106, loss = 0.23652273\n",
      "Iteration 107, loss = 0.23690547\n",
      "Iteration 108, loss = 0.23661577\n",
      "Iteration 109, loss = 0.23643375\n",
      "Iteration 110, loss = 0.23677918\n",
      "Iteration 111, loss = 0.23648142\n",
      "Iteration 112, loss = 0.23642650\n",
      "Iteration 113, loss = 0.23615996\n",
      "Iteration 114, loss = 0.23627012\n",
      "Iteration 115, loss = 0.23615220\n",
      "Iteration 116, loss = 0.23609276\n",
      "Iteration 117, loss = 0.23603898\n",
      "Iteration 118, loss = 0.23586368\n",
      "Iteration 119, loss = 0.23605496\n",
      "Iteration 120, loss = 0.23584204\n",
      "Iteration 121, loss = 0.23565198\n",
      "Iteration 122, loss = 0.23572395\n",
      "Iteration 123, loss = 0.23571467\n",
      "Iteration 124, loss = 0.23580421\n",
      "Iteration 125, loss = 0.23547953\n",
      "Iteration 126, loss = 0.23565677\n",
      "Iteration 127, loss = 0.23551365\n",
      "Iteration 128, loss = 0.23567408\n",
      "Iteration 129, loss = 0.23582403\n",
      "Iteration 130, loss = 0.23570711\n",
      "Iteration 131, loss = 0.23526946\n",
      "Iteration 132, loss = 0.23524541\n",
      "Iteration 133, loss = 0.23543171\n",
      "Iteration 134, loss = 0.23508987\n",
      "Iteration 135, loss = 0.23528378\n",
      "Iteration 136, loss = 0.23506936\n",
      "Iteration 137, loss = 0.23506570\n",
      "Iteration 138, loss = 0.23637620\n",
      "Iteration 139, loss = 0.23498121\n",
      "Iteration 140, loss = 0.23538457\n",
      "Iteration 141, loss = 0.23478814\n",
      "Iteration 142, loss = 0.23476062\n",
      "Iteration 143, loss = 0.23488904\n",
      "Iteration 144, loss = 0.23478329\n",
      "Iteration 145, loss = 0.23462974\n",
      "Iteration 146, loss = 0.23556382\n",
      "Iteration 147, loss = 0.23462511\n",
      "Iteration 148, loss = 0.23468912\n",
      "Iteration 149, loss = 0.23446736\n",
      "Iteration 150, loss = 0.23471309\n",
      "Iteration 151, loss = 0.23456153\n",
      "Iteration 152, loss = 0.23436780\n",
      "Iteration 153, loss = 0.23468007\n",
      "Iteration 154, loss = 0.23416177\n",
      "Iteration 155, loss = 0.23420342\n",
      "Iteration 156, loss = 0.23437979\n",
      "Iteration 157, loss = 0.23423366\n",
      "Iteration 158, loss = 0.23456890\n",
      "Iteration 159, loss = 0.23404670\n",
      "Iteration 160, loss = 0.23433758\n",
      "Iteration 161, loss = 0.23391382\n",
      "Iteration 162, loss = 0.23384004\n",
      "Iteration 163, loss = 0.23438754\n",
      "Iteration 164, loss = 0.23421574\n",
      "Iteration 165, loss = 0.23377906\n",
      "Iteration 166, loss = 0.23376557\n",
      "Iteration 167, loss = 0.23389367\n",
      "Iteration 168, loss = 0.23374048\n",
      "Iteration 169, loss = 0.23367391\n",
      "Iteration 170, loss = 0.23349648\n",
      "Iteration 171, loss = 0.23373898\n",
      "Iteration 172, loss = 0.23376608\n",
      "Iteration 173, loss = 0.23395416\n",
      "Iteration 174, loss = 0.23454589\n",
      "Iteration 175, loss = 0.23399379\n",
      "Iteration 176, loss = 0.23370966\n",
      "Iteration 177, loss = 0.23338968\n",
      "Iteration 178, loss = 0.23360009\n",
      "Iteration 179, loss = 0.23323030\n",
      "Iteration 180, loss = 0.23329630\n",
      "Iteration 181, loss = 0.23339673\n",
      "Iteration 182, loss = 0.23415754\n",
      "Iteration 183, loss = 0.23311883\n",
      "Iteration 184, loss = 0.23296495\n",
      "Iteration 185, loss = 0.23342189\n",
      "Iteration 186, loss = 0.23284534\n",
      "Iteration 187, loss = 0.23303547\n",
      "Iteration 188, loss = 0.23254226\n",
      "Iteration 189, loss = 0.23298324\n",
      "Iteration 190, loss = 0.23314107\n",
      "Iteration 191, loss = 0.23263323\n",
      "Iteration 192, loss = 0.23251064\n",
      "Iteration 193, loss = 0.23236244\n",
      "Iteration 194, loss = 0.23382604\n",
      "Iteration 195, loss = 0.23336829\n",
      "Iteration 196, loss = 0.23255216\n",
      "Iteration 197, loss = 0.23240121\n",
      "Iteration 198, loss = 0.23248081\n",
      "Iteration 199, loss = 0.23249299\n",
      "Iteration 200, loss = 0.23279794\n",
      "Iteration 201, loss = 0.23220958\n",
      "Iteration 202, loss = 0.23247805\n",
      "Iteration 203, loss = 0.23265439\n",
      "Iteration 204, loss = 0.23249736\n",
      "Iteration 205, loss = 0.23220596\n",
      "Iteration 206, loss = 0.23261724\n",
      "Iteration 207, loss = 0.23240660\n",
      "Iteration 208, loss = 0.23199655\n",
      "Iteration 209, loss = 0.23194460\n",
      "Iteration 210, loss = 0.23222577\n",
      "Iteration 211, loss = 0.23203816\n",
      "Iteration 212, loss = 0.23179365\n",
      "Iteration 213, loss = 0.23188368\n",
      "Iteration 214, loss = 0.23169823\n",
      "Iteration 215, loss = 0.23176315\n",
      "Iteration 216, loss = 0.23188350\n",
      "Iteration 217, loss = 0.23203623\n",
      "Iteration 218, loss = 0.23158051\n",
      "Iteration 219, loss = 0.23156607\n",
      "Iteration 220, loss = 0.23161447\n",
      "Iteration 221, loss = 0.23173132\n",
      "Iteration 222, loss = 0.23151293\n",
      "Iteration 223, loss = 0.23172297\n",
      "Iteration 224, loss = 0.23137278\n",
      "Iteration 225, loss = 0.23138999\n",
      "Iteration 226, loss = 0.23165186\n",
      "Iteration 227, loss = 0.23161828\n",
      "Iteration 228, loss = 0.23135136\n",
      "Iteration 229, loss = 0.23130330\n",
      "Iteration 230, loss = 0.23110278\n",
      "Iteration 231, loss = 0.23176435\n",
      "Iteration 232, loss = 0.23174357\n",
      "Iteration 233, loss = 0.23121609\n",
      "Iteration 234, loss = 0.23131232\n",
      "Iteration 235, loss = 0.23112786\n",
      "Iteration 236, loss = 0.23149840\n",
      "Iteration 237, loss = 0.23124536\n",
      "Iteration 238, loss = 0.23152191\n",
      "Iteration 239, loss = 0.23137420\n",
      "Iteration 240, loss = 0.23143612\n",
      "Iteration 241, loss = 0.23129715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "escala = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "clfNN = MLPClassifier(hidden_layer_sizes=(3, 10,10,10), alpha=1e-5,verbose=True, learning_rate=\"adaptive\",max_iter=500)\n",
    "clasNN = make_pipeline(escala, clfNN)\n",
    "clasNN.fit(X_train,y_train)\n",
    "clasNN.score(X_test,y_test)\n",
    "yp = clasNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673645320197044"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9367918902802623"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69361216\n",
      "Iteration 2, loss = 0.68981640\n",
      "Iteration 3, loss = 0.68763039\n",
      "Iteration 4, loss = 0.68442790\n",
      "Iteration 5, loss = 0.67911541\n",
      "Iteration 6, loss = 0.67219472\n",
      "Iteration 7, loss = 0.66606407\n",
      "Iteration 8, loss = 0.66075799\n",
      "Iteration 9, loss = 0.65737333\n",
      "Iteration 10, loss = 0.65543291\n",
      "Iteration 11, loss = 0.65340245\n",
      "Iteration 12, loss = 0.65154614\n",
      "Iteration 13, loss = 0.65052900\n",
      "Iteration 14, loss = 0.64936195\n",
      "Iteration 15, loss = 0.64857568\n",
      "Iteration 16, loss = 0.64822059\n",
      "Iteration 17, loss = 0.64684685\n",
      "Iteration 18, loss = 0.64635443\n",
      "Iteration 19, loss = 0.64550916\n",
      "Iteration 20, loss = 0.64559575\n",
      "Iteration 21, loss = 0.64482442\n",
      "Iteration 22, loss = 0.64410961\n",
      "Iteration 23, loss = 0.64353473\n",
      "Iteration 24, loss = 0.64324435\n",
      "Iteration 25, loss = 0.64266316\n",
      "Iteration 26, loss = 0.64221473\n",
      "Iteration 27, loss = 0.64210302\n",
      "Iteration 28, loss = 0.64189251\n",
      "Iteration 29, loss = 0.64163386\n",
      "Iteration 30, loss = 0.64183513\n",
      "Iteration 31, loss = 0.64075508\n",
      "Iteration 32, loss = 0.64099637\n",
      "Iteration 33, loss = 0.64065941\n",
      "Iteration 34, loss = 0.64001151\n",
      "Iteration 35, loss = 0.63982705\n",
      "Iteration 36, loss = 0.63957952\n",
      "Iteration 37, loss = 0.63948726\n",
      "Iteration 38, loss = 0.63920640\n",
      "Iteration 39, loss = 0.63931962\n",
      "Iteration 40, loss = 0.63874162\n",
      "Iteration 41, loss = 0.63812387\n",
      "Iteration 42, loss = 0.63766016\n",
      "Iteration 43, loss = 0.63745241\n",
      "Iteration 44, loss = 0.63705917\n",
      "Iteration 45, loss = 0.63643418\n",
      "Iteration 46, loss = 0.63640997\n",
      "Iteration 47, loss = 0.63597758\n",
      "Iteration 48, loss = 0.63540765\n",
      "Iteration 49, loss = 0.63507580\n",
      "Iteration 50, loss = 0.63491149\n",
      "Iteration 51, loss = 0.63446720\n",
      "Iteration 52, loss = 0.63434818\n",
      "Iteration 53, loss = 0.63380566\n",
      "Iteration 54, loss = 0.63375945\n",
      "Iteration 55, loss = 0.63325701\n",
      "Iteration 56, loss = 0.63258428\n",
      "Iteration 57, loss = 0.63240830\n",
      "Iteration 58, loss = 0.63179566\n",
      "Iteration 59, loss = 0.63162399\n",
      "Iteration 60, loss = 0.63150586\n",
      "Iteration 61, loss = 0.63100221\n",
      "Iteration 62, loss = 0.63113094\n",
      "Iteration 63, loss = 0.63080197\n",
      "Iteration 64, loss = 0.63003439\n",
      "Iteration 65, loss = 0.62993623\n",
      "Iteration 66, loss = 0.62956067\n",
      "Iteration 67, loss = 0.62929613\n",
      "Iteration 68, loss = 0.62955916\n",
      "Iteration 69, loss = 0.62898163\n",
      "Iteration 70, loss = 0.62867805\n",
      "Iteration 71, loss = 0.62842409\n",
      "Iteration 72, loss = 0.62845572\n",
      "Iteration 73, loss = 0.62839183\n",
      "Iteration 74, loss = 0.62742350\n",
      "Iteration 75, loss = 0.62793687\n",
      "Iteration 76, loss = 0.62789305\n",
      "Iteration 77, loss = 0.62689607\n",
      "Iteration 78, loss = 0.62770212\n",
      "Iteration 79, loss = 0.62656582\n",
      "Iteration 80, loss = 0.62689381\n",
      "Iteration 81, loss = 0.62638297\n",
      "Iteration 82, loss = 0.62608781\n",
      "Iteration 83, loss = 0.62556149\n",
      "Iteration 84, loss = 0.62599197\n",
      "Iteration 85, loss = 0.62516948\n",
      "Iteration 86, loss = 0.62561334\n",
      "Iteration 87, loss = 0.62486065\n",
      "Iteration 88, loss = 0.62505378\n",
      "Iteration 89, loss = 0.62502774\n",
      "Iteration 90, loss = 0.62462890\n",
      "Iteration 91, loss = 0.62423941\n",
      "Iteration 92, loss = 0.62418396\n",
      "Iteration 93, loss = 0.62433529\n",
      "Iteration 94, loss = 0.62379068\n",
      "Iteration 95, loss = 0.62368761\n",
      "Iteration 96, loss = 0.62334226\n",
      "Iteration 97, loss = 0.62363249\n",
      "Iteration 98, loss = 0.62315977\n",
      "Iteration 99, loss = 0.62385078\n",
      "Iteration 100, loss = 0.62282856\n",
      "Iteration 101, loss = 0.62285585\n",
      "Iteration 102, loss = 0.62260734\n",
      "Iteration 103, loss = 0.62252975\n",
      "Iteration 104, loss = 0.62243580\n",
      "Iteration 105, loss = 0.62240913\n",
      "Iteration 106, loss = 0.62224446\n",
      "Iteration 107, loss = 0.62242006\n",
      "Iteration 108, loss = 0.62230043\n",
      "Iteration 109, loss = 0.62181931\n",
      "Iteration 110, loss = 0.62241217\n",
      "Iteration 111, loss = 0.62225988\n",
      "Iteration 112, loss = 0.62226476\n",
      "Iteration 113, loss = 0.62149702\n",
      "Iteration 114, loss = 0.62128655\n",
      "Iteration 115, loss = 0.62089092\n",
      "Iteration 116, loss = 0.62082960\n",
      "Iteration 117, loss = 0.62153887\n",
      "Iteration 118, loss = 0.62118210\n",
      "Iteration 119, loss = 0.62055586\n",
      "Iteration 120, loss = 0.62090119\n",
      "Iteration 121, loss = 0.62038444\n",
      "Iteration 122, loss = 0.62030227\n",
      "Iteration 123, loss = 0.62049133\n",
      "Iteration 124, loss = 0.62057617\n",
      "Iteration 125, loss = 0.62020916\n",
      "Iteration 126, loss = 0.62030817\n",
      "Iteration 127, loss = 0.61996601\n",
      "Iteration 128, loss = 0.62048089\n",
      "Iteration 129, loss = 0.62012093\n",
      "Iteration 130, loss = 0.61947114\n",
      "Iteration 131, loss = 0.61962752\n",
      "Iteration 132, loss = 0.61932505\n",
      "Iteration 133, loss = 0.61962471\n",
      "Iteration 134, loss = 0.61879269\n",
      "Iteration 135, loss = 0.61926338\n",
      "Iteration 136, loss = 0.61846061\n",
      "Iteration 137, loss = 0.61879247\n",
      "Iteration 138, loss = 0.61828377\n",
      "Iteration 139, loss = 0.61809680\n",
      "Iteration 140, loss = 0.61837357\n",
      "Iteration 141, loss = 0.61802559\n",
      "Iteration 142, loss = 0.61761111\n",
      "Iteration 143, loss = 0.61760682\n",
      "Iteration 144, loss = 0.61728744\n",
      "Iteration 145, loss = 0.61746293\n",
      "Iteration 146, loss = 0.61713185\n",
      "Iteration 147, loss = 0.61695926\n",
      "Iteration 148, loss = 0.61745019\n",
      "Iteration 149, loss = 0.61675420\n",
      "Iteration 150, loss = 0.61675514\n",
      "Iteration 151, loss = 0.61661520\n",
      "Iteration 152, loss = 0.61622838\n",
      "Iteration 153, loss = 0.61676079\n",
      "Iteration 154, loss = 0.61647319\n",
      "Iteration 155, loss = 0.61551113\n",
      "Iteration 156, loss = 0.61575105\n",
      "Iteration 157, loss = 0.61520470\n",
      "Iteration 158, loss = 0.61516589\n",
      "Iteration 159, loss = 0.61524035\n",
      "Iteration 160, loss = 0.61484599\n",
      "Iteration 161, loss = 0.61476522\n",
      "Iteration 162, loss = 0.61534969\n",
      "Iteration 163, loss = 0.61435533\n",
      "Iteration 164, loss = 0.61402664\n",
      "Iteration 165, loss = 0.61409653\n",
      "Iteration 166, loss = 0.61407566\n",
      "Iteration 167, loss = 0.61371620\n",
      "Iteration 168, loss = 0.61365883\n",
      "Iteration 169, loss = 0.61379646\n",
      "Iteration 170, loss = 0.61313563\n",
      "Iteration 171, loss = 0.61310558\n",
      "Iteration 172, loss = 0.61237387\n",
      "Iteration 173, loss = 0.61282433\n",
      "Iteration 174, loss = 0.61281488\n",
      "Iteration 175, loss = 0.61227145\n",
      "Iteration 176, loss = 0.61168447\n",
      "Iteration 177, loss = 0.61140536\n",
      "Iteration 178, loss = 0.61104033\n",
      "Iteration 179, loss = 0.61061573\n",
      "Iteration 180, loss = 0.61091721\n",
      "Iteration 181, loss = 0.61108444\n",
      "Iteration 182, loss = 0.61138214\n",
      "Iteration 183, loss = 0.61038376\n",
      "Iteration 184, loss = 0.61139726\n",
      "Iteration 185, loss = 0.61036299\n",
      "Iteration 186, loss = 0.61042693\n",
      "Iteration 187, loss = 0.60938456\n",
      "Iteration 188, loss = 0.60952976\n",
      "Iteration 189, loss = 0.60989778\n",
      "Iteration 190, loss = 0.60947922\n",
      "Iteration 191, loss = 0.60971215\n",
      "Iteration 192, loss = 0.60926867\n",
      "Iteration 193, loss = 0.60913773\n",
      "Iteration 194, loss = 0.60989947\n",
      "Iteration 195, loss = 0.60868674\n",
      "Iteration 196, loss = 0.60914188\n",
      "Iteration 197, loss = 0.60859565\n",
      "Iteration 198, loss = 0.60820395\n",
      "Iteration 199, loss = 0.60817010\n",
      "Iteration 200, loss = 0.60793944\n",
      "Iteration 201, loss = 0.60734597\n",
      "Iteration 202, loss = 0.60786783\n",
      "Iteration 203, loss = 0.60776010\n",
      "Iteration 204, loss = 0.60674414\n",
      "Iteration 205, loss = 0.60727960\n",
      "Iteration 206, loss = 0.60820679\n",
      "Iteration 207, loss = 0.60660703\n",
      "Iteration 208, loss = 0.60642978\n",
      "Iteration 209, loss = 0.60651673\n",
      "Iteration 210, loss = 0.60643227\n",
      "Iteration 211, loss = 0.60561731\n",
      "Iteration 212, loss = 0.60570875\n",
      "Iteration 213, loss = 0.60482258\n",
      "Iteration 214, loss = 0.60536265\n",
      "Iteration 215, loss = 0.60516728\n",
      "Iteration 216, loss = 0.60536198\n",
      "Iteration 217, loss = 0.60458944\n",
      "Iteration 218, loss = 0.60485864\n",
      "Iteration 219, loss = 0.60481913\n",
      "Iteration 220, loss = 0.60391870\n",
      "Iteration 221, loss = 0.60480698\n",
      "Iteration 222, loss = 0.60500622\n",
      "Iteration 223, loss = 0.60539017\n",
      "Iteration 224, loss = 0.60497264\n",
      "Iteration 225, loss = 0.60366989\n",
      "Iteration 226, loss = 0.60333319\n",
      "Iteration 227, loss = 0.60304040\n",
      "Iteration 228, loss = 0.60365416\n",
      "Iteration 229, loss = 0.60289651\n",
      "Iteration 230, loss = 0.60425434\n",
      "Iteration 231, loss = 0.60280951\n",
      "Iteration 232, loss = 0.60256685\n",
      "Iteration 233, loss = 0.60259892\n",
      "Iteration 234, loss = 0.60280703\n",
      "Iteration 235, loss = 0.60142792\n",
      "Iteration 236, loss = 0.60113003\n",
      "Iteration 237, loss = 0.60201490\n",
      "Iteration 238, loss = 0.60235882\n",
      "Iteration 239, loss = 0.60142157\n",
      "Iteration 240, loss = 0.60189670\n",
      "Iteration 241, loss = 0.60029821\n",
      "Iteration 242, loss = 0.60037747\n",
      "Iteration 243, loss = 0.60202036\n",
      "Iteration 244, loss = 0.60030209\n",
      "Iteration 245, loss = 0.60049685\n",
      "Iteration 246, loss = 0.60050168\n",
      "Iteration 247, loss = 0.59973399\n",
      "Iteration 248, loss = 0.60002464\n",
      "Iteration 249, loss = 0.60012851\n",
      "Iteration 250, loss = 0.59887946\n",
      "Iteration 251, loss = 0.60061762\n",
      "Iteration 252, loss = 0.59909415\n",
      "Iteration 253, loss = 0.59862500\n",
      "Iteration 254, loss = 0.59891759\n",
      "Iteration 255, loss = 0.59868883\n",
      "Iteration 256, loss = 0.59780827\n",
      "Iteration 257, loss = 0.59817682\n",
      "Iteration 258, loss = 0.59881761\n",
      "Iteration 259, loss = 0.59870813\n",
      "Iteration 260, loss = 0.59724956\n",
      "Iteration 261, loss = 0.59821042\n",
      "Iteration 262, loss = 0.59776610\n",
      "Iteration 263, loss = 0.59834893\n",
      "Iteration 264, loss = 0.59769804\n",
      "Iteration 265, loss = 0.59738145\n",
      "Iteration 266, loss = 0.59710680\n",
      "Iteration 267, loss = 0.59748689\n",
      "Iteration 268, loss = 0.59692697\n",
      "Iteration 269, loss = 0.59715838\n",
      "Iteration 270, loss = 0.59681890\n",
      "Iteration 271, loss = 0.59660833\n",
      "Iteration 272, loss = 0.59627397\n",
      "Iteration 273, loss = 0.59677930\n",
      "Iteration 274, loss = 0.59668084\n",
      "Iteration 275, loss = 0.59588472\n",
      "Iteration 276, loss = 0.59555878\n",
      "Iteration 277, loss = 0.59633358\n",
      "Iteration 278, loss = 0.59590718\n",
      "Iteration 279, loss = 0.59569031\n",
      "Iteration 280, loss = 0.59625906\n",
      "Iteration 281, loss = 0.59587962\n",
      "Iteration 282, loss = 0.59573682\n",
      "Iteration 283, loss = 0.59599080\n",
      "Iteration 284, loss = 0.59503394\n",
      "Iteration 285, loss = 0.59470329\n",
      "Iteration 286, loss = 0.59473294\n",
      "Iteration 287, loss = 0.59349731\n",
      "Iteration 288, loss = 0.59516706\n",
      "Iteration 289, loss = 0.59404912\n",
      "Iteration 290, loss = 0.59556745\n",
      "Iteration 291, loss = 0.59468442\n",
      "Iteration 292, loss = 0.59475431\n",
      "Iteration 293, loss = 0.59455417\n",
      "Iteration 294, loss = 0.59379337\n",
      "Iteration 295, loss = 0.59402994\n",
      "Iteration 296, loss = 0.59420920\n",
      "Iteration 297, loss = 0.59469312\n",
      "Iteration 298, loss = 0.59298254\n",
      "Iteration 299, loss = 0.59346983\n",
      "Iteration 300, loss = 0.59515852\n",
      "Iteration 301, loss = 0.59278491\n",
      "Iteration 302, loss = 0.59344788\n",
      "Iteration 303, loss = 0.59287113\n",
      "Iteration 304, loss = 0.59311219\n",
      "Iteration 305, loss = 0.59266150\n",
      "Iteration 306, loss = 0.59191933\n",
      "Iteration 307, loss = 0.59181055\n",
      "Iteration 308, loss = 0.59353982\n",
      "Iteration 309, loss = 0.59393189\n",
      "Iteration 310, loss = 0.59227536\n",
      "Iteration 311, loss = 0.59204337\n",
      "Iteration 312, loss = 0.59281661\n",
      "Iteration 313, loss = 0.59307685\n",
      "Iteration 314, loss = 0.59184675\n",
      "Iteration 315, loss = 0.59083903\n",
      "Iteration 316, loss = 0.59282039\n",
      "Iteration 317, loss = 0.59154447\n",
      "Iteration 318, loss = 0.59240443\n",
      "Iteration 319, loss = 0.59188610\n",
      "Iteration 320, loss = 0.59086591\n",
      "Iteration 321, loss = 0.59217533\n",
      "Iteration 322, loss = 0.59354810\n",
      "Iteration 323, loss = 0.59132731\n",
      "Iteration 324, loss = 0.59203107\n",
      "Iteration 325, loss = 0.59035228\n",
      "Iteration 326, loss = 0.58990117\n",
      "Iteration 327, loss = 0.59059732\n",
      "Iteration 328, loss = 0.59041301\n",
      "Iteration 329, loss = 0.59102660\n",
      "Iteration 330, loss = 0.59099940\n",
      "Iteration 331, loss = 0.59188766\n",
      "Iteration 332, loss = 0.59094992\n",
      "Iteration 333, loss = 0.58900833\n",
      "Iteration 334, loss = 0.59037643\n",
      "Iteration 335, loss = 0.58936356\n",
      "Iteration 336, loss = 0.59025957\n",
      "Iteration 337, loss = 0.59014532\n",
      "Iteration 338, loss = 0.58886800\n",
      "Iteration 339, loss = 0.58993490\n",
      "Iteration 340, loss = 0.58982579\n",
      "Iteration 341, loss = 0.58938476\n",
      "Iteration 342, loss = 0.58956252\n",
      "Iteration 343, loss = 0.58949976\n",
      "Iteration 344, loss = 0.58891160\n",
      "Iteration 345, loss = 0.58860874\n",
      "Iteration 346, loss = 0.58934019\n",
      "Iteration 347, loss = 0.59060245\n",
      "Iteration 348, loss = 0.58873113\n",
      "Iteration 349, loss = 0.58975952\n",
      "Iteration 350, loss = 0.58859911\n",
      "Iteration 351, loss = 0.58798687\n",
      "Iteration 352, loss = 0.58871074\n",
      "Iteration 353, loss = 0.58792940\n",
      "Iteration 354, loss = 0.58777065\n",
      "Iteration 355, loss = 0.58772434\n",
      "Iteration 356, loss = 0.58659096\n",
      "Iteration 357, loss = 0.58763483\n",
      "Iteration 358, loss = 0.58701578\n",
      "Iteration 359, loss = 0.58746193\n",
      "Iteration 360, loss = 0.58788739\n",
      "Iteration 361, loss = 0.58747062\n",
      "Iteration 362, loss = 0.58737091\n",
      "Iteration 363, loss = 0.58737933\n",
      "Iteration 364, loss = 0.58700698\n",
      "Iteration 365, loss = 0.58798615\n",
      "Iteration 366, loss = 0.58739826\n",
      "Iteration 367, loss = 0.58732028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "y = modificado.COMPARECEU_AGENDAMENTO_EXAME\n",
    "X = modificado.drop('COMPARECEU_AGENDAMENTO_EXAME', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "NAO = X[X.COMPARECEU_AGENDAMENTO_EXAME==0]\n",
    "SIM = X[X.COMPARECEU_AGENDAMENTO_EXAME==1]\n",
    "\n",
    "NAO_upsampled = resample(NAO,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(SIM), # match number in majority class\n",
    "                          random_state=42)\n",
    "\n",
    "upsampled = pd.concat([SIM, NAO_upsampled])\n",
    "upsampled.COMPARECEU_AGENDAMENTO_EXAME.value_counts()\n",
    "\n",
    "y_train_md = np.array(upsampled.COMPARECEU_AGENDAMENTO_EXAME)\n",
    "X_train_md = np.array(upsampled.drop('COMPARECEU_AGENDAMENTO_EXAME', axis=1))\n",
    "\n",
    "\n",
    "escala = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "clfNN = MLPClassifier(hidden_layer_sizes=(3, 10,10,10), alpha=1e-5,verbose=True, learning_rate=\"adaptive\",max_iter=500)\n",
    "clasNN = make_pipeline(escala, clfNN)\n",
    "clasNN.fit(X_train_md,y_train_md)\n",
    "clasNN.score(np.array(X_test),np.array(y_test))\n",
    "yp = clasNN.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729323308270677"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp = clasNN.predict(np.array(X_test))\n",
    "f1_score(yp,np.array(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "574e93fb58bda07035505f46e2c46eff280992c7276cddd05cbfe2fed6edb448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
