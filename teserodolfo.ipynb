{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "original = pd.read_csv(\"dados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "modificado = pd.DataFrame()\n",
    "for x in original:\n",
    "    if original[x].dtypes==object:\n",
    "        modificado[x] = le.fit_transform(np.array(original[x]))\n",
    "    else:\n",
    "        modificado[x] = np.array(original[x])\n",
    "\n",
    "clm = list(modificado.columns)\n",
    "X = np.array(modificado[clm[0:-1]])\n",
    "y = np.array(modificado[clm[-1]])\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "escala = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "clfNN = MLPClassifier(hidden_layer_sizes=(3, 10,10,10), alpha=1e-5,verbose=True, learning_rate=\"adaptive\",max_iter=500)\n",
    "clasNN = make_pipeline(escala, clfNN)\n",
    "clasNN.fit(X_train,y_train)\n",
    "clasNN.score(X_test,y_test)\n",
    "yp = clasNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72851124\n",
      "Iteration 2, loss = 0.70275743\n",
      "Iteration 3, loss = 0.69059254\n",
      "Iteration 4, loss = 0.68745636\n",
      "Iteration 5, loss = 0.68388690\n",
      "Iteration 6, loss = 0.67870323\n",
      "Iteration 7, loss = 0.67172275\n",
      "Iteration 8, loss = 0.66624122\n",
      "Iteration 9, loss = 0.66183168\n",
      "Iteration 10, loss = 0.65962970\n",
      "Iteration 11, loss = 0.65830348\n",
      "Iteration 12, loss = 0.65730169\n",
      "Iteration 13, loss = 0.65623201\n",
      "Iteration 14, loss = 0.65495652\n",
      "Iteration 15, loss = 0.65368901\n",
      "Iteration 16, loss = 0.65244944\n",
      "Iteration 17, loss = 0.65130142\n",
      "Iteration 18, loss = 0.65029092\n",
      "Iteration 19, loss = 0.64901062\n",
      "Iteration 20, loss = 0.64899104\n",
      "Iteration 21, loss = 0.64690093\n",
      "Iteration 22, loss = 0.64587004\n",
      "Iteration 23, loss = 0.64471503\n",
      "Iteration 24, loss = 0.64357314\n",
      "Iteration 25, loss = 0.64255999\n",
      "Iteration 26, loss = 0.64187074\n",
      "Iteration 27, loss = 0.64069878\n",
      "Iteration 28, loss = 0.64005763\n",
      "Iteration 29, loss = 0.63909391\n",
      "Iteration 30, loss = 0.63860926\n",
      "Iteration 31, loss = 0.63785651\n",
      "Iteration 32, loss = 0.63651779\n",
      "Iteration 33, loss = 0.63611562\n",
      "Iteration 34, loss = 0.63560969\n",
      "Iteration 35, loss = 0.63470808\n",
      "Iteration 36, loss = 0.63453383\n",
      "Iteration 37, loss = 0.63359258\n",
      "Iteration 38, loss = 0.63289963\n",
      "Iteration 39, loss = 0.63348494\n",
      "Iteration 40, loss = 0.63205288\n",
      "Iteration 41, loss = 0.63125958\n",
      "Iteration 42, loss = 0.63101092\n",
      "Iteration 43, loss = 0.63051455\n",
      "Iteration 44, loss = 0.62999736\n",
      "Iteration 45, loss = 0.62892259\n",
      "Iteration 46, loss = 0.62903202\n",
      "Iteration 47, loss = 0.62820116\n",
      "Iteration 48, loss = 0.62741658\n",
      "Iteration 49, loss = 0.62696906\n",
      "Iteration 50, loss = 0.62620840\n",
      "Iteration 51, loss = 0.62579847\n",
      "Iteration 52, loss = 0.62558905\n",
      "Iteration 53, loss = 0.62565932\n",
      "Iteration 54, loss = 0.62470001\n",
      "Iteration 55, loss = 0.62505812\n",
      "Iteration 56, loss = 0.62396861\n",
      "Iteration 57, loss = 0.62383054\n",
      "Iteration 58, loss = 0.62302526\n",
      "Iteration 59, loss = 0.62292488\n",
      "Iteration 60, loss = 0.62208575\n",
      "Iteration 61, loss = 0.62105496\n",
      "Iteration 62, loss = 0.62059671\n",
      "Iteration 63, loss = 0.61961219\n",
      "Iteration 64, loss = 0.61891304\n",
      "Iteration 65, loss = 0.61800619\n",
      "Iteration 66, loss = 0.61677829\n",
      "Iteration 67, loss = 0.61682776\n",
      "Iteration 68, loss = 0.61569117\n",
      "Iteration 69, loss = 0.61465896\n",
      "Iteration 70, loss = 0.61428217\n",
      "Iteration 71, loss = 0.61360730\n",
      "Iteration 72, loss = 0.61318903\n",
      "Iteration 73, loss = 0.61220249\n",
      "Iteration 74, loss = 0.61189743\n",
      "Iteration 75, loss = 0.61072283\n",
      "Iteration 76, loss = 0.61047418\n",
      "Iteration 77, loss = 0.61042878\n",
      "Iteration 78, loss = 0.60960438\n",
      "Iteration 79, loss = 0.60960902\n",
      "Iteration 80, loss = 0.60925388\n",
      "Iteration 81, loss = 0.60909103\n",
      "Iteration 82, loss = 0.60861695\n",
      "Iteration 83, loss = 0.60758371\n",
      "Iteration 84, loss = 0.60756501\n",
      "Iteration 85, loss = 0.60793547\n",
      "Iteration 86, loss = 0.60723934\n",
      "Iteration 87, loss = 0.60640493\n",
      "Iteration 88, loss = 0.60649984\n",
      "Iteration 89, loss = 0.60644175\n",
      "Iteration 90, loss = 0.60571441\n",
      "Iteration 91, loss = 0.60534469\n",
      "Iteration 92, loss = 0.60569916\n",
      "Iteration 93, loss = 0.60524480\n",
      "Iteration 94, loss = 0.60555346\n",
      "Iteration 95, loss = 0.60438841\n",
      "Iteration 96, loss = 0.60438725\n",
      "Iteration 97, loss = 0.60483339\n",
      "Iteration 98, loss = 0.60451212\n",
      "Iteration 99, loss = 0.60404096\n",
      "Iteration 100, loss = 0.60380643\n",
      "Iteration 101, loss = 0.60362164\n",
      "Iteration 102, loss = 0.60361859\n",
      "Iteration 103, loss = 0.60352000\n",
      "Iteration 104, loss = 0.60276500\n",
      "Iteration 105, loss = 0.60313023\n",
      "Iteration 106, loss = 0.60370516\n",
      "Iteration 107, loss = 0.60264875\n",
      "Iteration 108, loss = 0.60228034\n",
      "Iteration 109, loss = 0.60254155\n",
      "Iteration 110, loss = 0.60264372\n",
      "Iteration 111, loss = 0.60239301\n",
      "Iteration 112, loss = 0.60238322\n",
      "Iteration 113, loss = 0.60200504\n",
      "Iteration 114, loss = 0.60214478\n",
      "Iteration 115, loss = 0.60181697\n",
      "Iteration 116, loss = 0.60225766\n",
      "Iteration 117, loss = 0.60116512\n",
      "Iteration 118, loss = 0.60182332\n",
      "Iteration 119, loss = 0.60160262\n",
      "Iteration 120, loss = 0.60123741\n",
      "Iteration 121, loss = 0.60073578\n",
      "Iteration 122, loss = 0.60053436\n",
      "Iteration 123, loss = 0.60092372\n",
      "Iteration 124, loss = 0.60032085\n",
      "Iteration 125, loss = 0.60085819\n",
      "Iteration 126, loss = 0.60062410\n",
      "Iteration 127, loss = 0.59975218\n",
      "Iteration 128, loss = 0.59986294\n",
      "Iteration 129, loss = 0.59996966\n",
      "Iteration 130, loss = 0.59926130\n",
      "Iteration 131, loss = 0.59976447\n",
      "Iteration 132, loss = 0.59944924\n",
      "Iteration 133, loss = 0.59980160\n",
      "Iteration 134, loss = 0.59866964\n",
      "Iteration 135, loss = 0.59902454\n",
      "Iteration 136, loss = 0.59830892\n",
      "Iteration 137, loss = 0.59813461\n",
      "Iteration 138, loss = 0.59789728\n",
      "Iteration 139, loss = 0.59892932\n",
      "Iteration 140, loss = 0.59758557\n",
      "Iteration 141, loss = 0.59803572\n",
      "Iteration 142, loss = 0.59715586\n",
      "Iteration 143, loss = 0.59746803\n",
      "Iteration 144, loss = 0.59770220\n",
      "Iteration 145, loss = 0.59732716\n",
      "Iteration 146, loss = 0.59671267\n",
      "Iteration 147, loss = 0.59707425\n",
      "Iteration 148, loss = 0.59645400\n",
      "Iteration 149, loss = 0.59665971\n",
      "Iteration 150, loss = 0.59721037\n",
      "Iteration 151, loss = 0.59670133\n",
      "Iteration 152, loss = 0.59663742\n",
      "Iteration 153, loss = 0.59642490\n",
      "Iteration 154, loss = 0.59567756\n",
      "Iteration 155, loss = 0.59647842\n",
      "Iteration 156, loss = 0.59606309\n",
      "Iteration 157, loss = 0.59573681\n",
      "Iteration 158, loss = 0.59559164\n",
      "Iteration 159, loss = 0.59568776\n",
      "Iteration 160, loss = 0.59602694\n",
      "Iteration 161, loss = 0.59463788\n",
      "Iteration 162, loss = 0.59406444\n",
      "Iteration 163, loss = 0.59419385\n",
      "Iteration 164, loss = 0.59494370\n",
      "Iteration 165, loss = 0.59442663\n",
      "Iteration 166, loss = 0.59350689\n",
      "Iteration 167, loss = 0.59335223\n",
      "Iteration 168, loss = 0.59383574\n",
      "Iteration 169, loss = 0.59311613\n",
      "Iteration 170, loss = 0.59336033\n",
      "Iteration 171, loss = 0.59349511\n",
      "Iteration 172, loss = 0.59306400\n",
      "Iteration 173, loss = 0.59252366\n",
      "Iteration 174, loss = 0.59236793\n",
      "Iteration 175, loss = 0.59269805\n",
      "Iteration 176, loss = 0.59286366\n",
      "Iteration 177, loss = 0.59231488\n",
      "Iteration 178, loss = 0.59215454\n",
      "Iteration 179, loss = 0.59223736\n",
      "Iteration 180, loss = 0.59246968\n",
      "Iteration 181, loss = 0.59188506\n",
      "Iteration 182, loss = 0.59227692\n",
      "Iteration 183, loss = 0.59165985\n",
      "Iteration 184, loss = 0.59185197\n",
      "Iteration 185, loss = 0.59091446\n",
      "Iteration 186, loss = 0.59172994\n",
      "Iteration 187, loss = 0.59124174\n",
      "Iteration 188, loss = 0.59214519\n",
      "Iteration 189, loss = 0.59142638\n",
      "Iteration 190, loss = 0.59120254\n",
      "Iteration 191, loss = 0.59097272\n",
      "Iteration 192, loss = 0.59083646\n",
      "Iteration 193, loss = 0.59108758\n",
      "Iteration 194, loss = 0.59097897\n",
      "Iteration 195, loss = 0.59065946\n",
      "Iteration 196, loss = 0.59036665\n",
      "Iteration 197, loss = 0.59004819\n",
      "Iteration 198, loss = 0.59044061\n",
      "Iteration 199, loss = 0.59084237\n",
      "Iteration 200, loss = 0.59068395\n",
      "Iteration 201, loss = 0.58996722\n",
      "Iteration 202, loss = 0.59066813\n",
      "Iteration 203, loss = 0.59027220\n",
      "Iteration 204, loss = 0.58973466\n",
      "Iteration 205, loss = 0.59035274\n",
      "Iteration 206, loss = 0.58945544\n",
      "Iteration 207, loss = 0.58930957\n",
      "Iteration 208, loss = 0.58973692\n",
      "Iteration 209, loss = 0.58903319\n",
      "Iteration 210, loss = 0.58884828\n",
      "Iteration 211, loss = 0.58958861\n",
      "Iteration 212, loss = 0.58913312\n",
      "Iteration 213, loss = 0.58851995\n",
      "Iteration 214, loss = 0.58848715\n",
      "Iteration 215, loss = 0.58852616\n",
      "Iteration 216, loss = 0.58865352\n",
      "Iteration 217, loss = 0.58790636\n",
      "Iteration 218, loss = 0.58824669\n",
      "Iteration 219, loss = 0.58884485\n",
      "Iteration 220, loss = 0.58760411\n",
      "Iteration 221, loss = 0.58796117\n",
      "Iteration 222, loss = 0.58887951\n",
      "Iteration 223, loss = 0.58855823\n",
      "Iteration 224, loss = 0.58779699\n",
      "Iteration 225, loss = 0.58831567\n",
      "Iteration 226, loss = 0.58794000\n",
      "Iteration 227, loss = 0.58748229\n",
      "Iteration 228, loss = 0.58699513\n",
      "Iteration 229, loss = 0.58679138\n",
      "Iteration 230, loss = 0.58680141\n",
      "Iteration 231, loss = 0.58704810\n",
      "Iteration 232, loss = 0.58619514\n",
      "Iteration 233, loss = 0.58692927\n",
      "Iteration 234, loss = 0.58707210\n",
      "Iteration 235, loss = 0.58659342\n",
      "Iteration 236, loss = 0.58637887\n",
      "Iteration 237, loss = 0.58670311\n",
      "Iteration 238, loss = 0.58580403\n",
      "Iteration 239, loss = 0.58642711\n",
      "Iteration 240, loss = 0.58654075\n",
      "Iteration 241, loss = 0.58647522\n",
      "Iteration 242, loss = 0.58590763\n",
      "Iteration 243, loss = 0.58585540\n",
      "Iteration 244, loss = 0.58609008\n",
      "Iteration 245, loss = 0.58634938\n",
      "Iteration 246, loss = 0.58537137\n",
      "Iteration 247, loss = 0.58485230\n",
      "Iteration 248, loss = 0.58577187\n",
      "Iteration 249, loss = 0.58478323\n",
      "Iteration 250, loss = 0.58455458\n",
      "Iteration 251, loss = 0.58443784\n",
      "Iteration 252, loss = 0.58484253\n",
      "Iteration 253, loss = 0.58473109\n",
      "Iteration 254, loss = 0.58560326\n",
      "Iteration 255, loss = 0.58421918\n",
      "Iteration 256, loss = 0.58405171\n",
      "Iteration 257, loss = 0.58377600\n",
      "Iteration 258, loss = 0.58415090\n",
      "Iteration 259, loss = 0.58315658\n",
      "Iteration 260, loss = 0.58389795\n",
      "Iteration 261, loss = 0.58370365\n",
      "Iteration 262, loss = 0.58455731\n",
      "Iteration 263, loss = 0.58377828\n",
      "Iteration 264, loss = 0.58406877\n",
      "Iteration 265, loss = 0.58252123\n",
      "Iteration 266, loss = 0.58251906\n",
      "Iteration 267, loss = 0.58227206\n",
      "Iteration 268, loss = 0.58216840\n",
      "Iteration 269, loss = 0.58165131\n",
      "Iteration 270, loss = 0.58252287\n",
      "Iteration 271, loss = 0.58206337\n",
      "Iteration 272, loss = 0.58184162\n",
      "Iteration 273, loss = 0.58189745\n",
      "Iteration 274, loss = 0.58243627\n",
      "Iteration 275, loss = 0.58126964\n",
      "Iteration 276, loss = 0.58240854\n",
      "Iteration 277, loss = 0.58162531\n",
      "Iteration 278, loss = 0.58143340\n",
      "Iteration 279, loss = 0.58105402\n",
      "Iteration 280, loss = 0.58103367\n",
      "Iteration 281, loss = 0.58069017\n",
      "Iteration 282, loss = 0.58083707\n",
      "Iteration 283, loss = 0.58092315\n",
      "Iteration 284, loss = 0.58018702\n",
      "Iteration 285, loss = 0.58034574\n",
      "Iteration 286, loss = 0.57964585\n",
      "Iteration 287, loss = 0.58039823\n",
      "Iteration 288, loss = 0.57962552\n",
      "Iteration 289, loss = 0.57970517\n",
      "Iteration 290, loss = 0.57953813\n",
      "Iteration 291, loss = 0.57871365\n",
      "Iteration 292, loss = 0.57996169\n",
      "Iteration 293, loss = 0.57990506\n",
      "Iteration 294, loss = 0.58038499\n",
      "Iteration 295, loss = 0.57975299\n",
      "Iteration 296, loss = 0.57891172\n",
      "Iteration 297, loss = 0.57806147\n",
      "Iteration 298, loss = 0.57852058\n",
      "Iteration 299, loss = 0.57826086\n",
      "Iteration 300, loss = 0.57821533\n",
      "Iteration 301, loss = 0.57856450\n",
      "Iteration 302, loss = 0.57777436\n",
      "Iteration 303, loss = 0.57903520\n",
      "Iteration 304, loss = 0.57931327\n",
      "Iteration 305, loss = 0.57883572\n",
      "Iteration 306, loss = 0.57808978\n",
      "Iteration 307, loss = 0.57857041\n",
      "Iteration 308, loss = 0.57800875\n",
      "Iteration 309, loss = 0.57842289\n",
      "Iteration 310, loss = 0.57878024\n",
      "Iteration 311, loss = 0.57708702\n",
      "Iteration 312, loss = 0.57724198\n",
      "Iteration 313, loss = 0.57735630\n",
      "Iteration 314, loss = 0.57716871\n",
      "Iteration 315, loss = 0.57651555\n",
      "Iteration 316, loss = 0.57724675\n",
      "Iteration 317, loss = 0.57657111\n",
      "Iteration 318, loss = 0.57637468\n",
      "Iteration 319, loss = 0.57570088\n",
      "Iteration 320, loss = 0.57711695\n",
      "Iteration 321, loss = 0.57646723\n",
      "Iteration 322, loss = 0.57837268\n",
      "Iteration 323, loss = 0.57719181\n",
      "Iteration 324, loss = 0.57625367\n",
      "Iteration 325, loss = 0.57568339\n",
      "Iteration 326, loss = 0.57865115\n",
      "Iteration 327, loss = 0.57703527\n",
      "Iteration 328, loss = 0.57608831\n",
      "Iteration 329, loss = 0.57695432\n",
      "Iteration 330, loss = 0.57674987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7328292375551354 0.5954198473282443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "modificado = pd.DataFrame()\n",
    "for x in original:\n",
    "    if original[x].dtypes==object:\n",
    "        modificado[x] = le.fit_transform(np.array(original[x]))\n",
    "    else:\n",
    "        modificado[x] = np.array(original[x])\n",
    "\n",
    "y = modificado.COMPARECEU_AGENDAMENTO_EXAME\n",
    "X = modificado.drop('COMPARECEU_AGENDAMENTO_EXAME', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "NAO = X[X.COMPARECEU_AGENDAMENTO_EXAME==0]\n",
    "SIM = X[X.COMPARECEU_AGENDAMENTO_EXAME==1]\n",
    "\n",
    "NAO_upsampled = resample(NAO,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(SIM), # match number in majority class\n",
    "                          random_state=42)\n",
    "\n",
    "upsampled = pd.concat([SIM, NAO_upsampled])\n",
    "upsampled.COMPARECEU_AGENDAMENTO_EXAME.value_counts()\n",
    "\n",
    "y_train_md = np.array(upsampled.COMPARECEU_AGENDAMENTO_EXAME)\n",
    "X_train_md = np.array(upsampled.drop('COMPARECEU_AGENDAMENTO_EXAME', axis=1))\n",
    "\n",
    "\n",
    "escala = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "clfNN = MLPClassifier(hidden_layer_sizes=(3, 10,10,10), alpha=1e-5,verbose=True, learning_rate=\"adaptive\",max_iter=500)\n",
    "clasNN = make_pipeline(escala, clfNN)\n",
    "clasNN.fit(X_train_md,y_train_md)\n",
    "sc = clasNN.score(np.array(X_test),np.array(y_test))\n",
    "yp = clasNN.predict(np.array(X_test))\n",
    "f1sc = f1_score(yp,np.array(y_test))\n",
    "print(f1sc,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "574e93fb58bda07035505f46e2c46eff280992c7276cddd05cbfe2fed6edb448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
